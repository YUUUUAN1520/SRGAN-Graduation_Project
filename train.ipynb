{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "english-fairy",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import get_generator, get_discriminator, get_gan\n",
    "from vgg19_loss import VGG_LOSS\n",
    "from Utils import plot_generated_images, denormalize\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tqdm import tqdm\n",
    "import numpy as np \n",
    "np.random.seed(10)\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "image_shape = (384,384,3)\n",
    "downscale_factor = 4\n",
    "__dataset_name__ = 'DIV2K_train_HR'\n",
    "dataset_tensor = np.load('./datasets/'+ __dataset_name__ + '_tensor.npz')\n",
    "\n",
    "scaled_image_shape = (image_shape[0] // downscale_factor, \n",
    "                      image_shape[1] // downscale_factor, \n",
    "                      image_shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fatal-definition",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define optimizer\n",
    "def get_optimizer():\n",
    "    adam = Adam(lr=1e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "    return adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fd78625",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(path, trained_generator, trained_discriminator):\n",
    "    trained_generator.save(path + '/Generator.h5')\n",
    "    \n",
    "    trained_discriminator.trainable = True\n",
    "    trained_discriminator.save(path + '/Discriminator.h5')\n",
    "    \n",
    "    \n",
    "def load_model(path):\n",
    "    loss = VGG_LOSS(image_shape)\n",
    "    Optimizer = get_optimizer()\n",
    "    \n",
    "    trained_generator = tf.keras.models.load_model(path + '/Generator.h5', compile=False)\n",
    "    trained_discriminator = tf.keras.models.load_model(path + '/Discriminator.h5', compile=False)\n",
    "    trained_discriminator.trainable = True\n",
    "    \n",
    "    return [trained_generator, trained_discriminator]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "quarterly-chess",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs, batch_size, show_interval=5, give_model=None, have_trained_epochs=0):\n",
    "    # load data\n",
    "    imgs_hr, imgs_lr = dataset_tensor['imgs_hr'], dataset_tensor['imgs_lr']\n",
    "    imgs_size = imgs_hr.shape[0]\n",
    "    \n",
    "    # define loss function.\n",
    "    loss = VGG_LOSS(image_shape)\n",
    "    \n",
    "    # define optimizer function.\n",
    "    Optimizer = get_optimizer() \n",
    "    \n",
    "    if give_model == None:  # initialize model.\n",
    "        Generator = get_generator(scaled_image_shape,show_model=False)\n",
    "        Discriminator = get_discriminator(image_shape,show_model=False)\n",
    "\n",
    "    else:   # load model.\n",
    "        Generator = give_model[0]\n",
    "        Discriminator = give_model[1]\n",
    "        \n",
    "    Generator.compile(loss=loss.vgg_loss, optimizer=Optimizer) \n",
    "    Discriminator.compile(loss=\"binary_crossentropy\", optimizer=Optimizer)\n",
    "    \n",
    "    GAN = get_gan(Discriminator, Generator, generator_input_shape=scaled_image_shape, vgg_loss=loss.vgg_loss)\n",
    "    GAN.compile(loss=[loss.vgg_loss,\"binary_crossentropy\"],\n",
    "                loss_weights=[1.,1e-3],\n",
    "                optimizer=Optimizer)\n",
    "    \n",
    "    for e in range(1,epochs+1):\n",
    "        print(\"-\" * 15 + \"Epoch \" + str(e) + \" of \" + str(epochs) + \"-\" * 15)\n",
    "        sum_GAN_loss = 0\n",
    "        sum_d_loss = 0\n",
    "        for _ in tqdm(range(imgs_size // batch_size)):\n",
    "\n",
    "            # -----------------------\n",
    "            # training Discriminator\n",
    "            # -----------------------\n",
    "            selected_index = np.random.choice(imgs_size,batch_size)\n",
    "            batch_hr, batch_lr = imgs_hr[selected_index], imgs_lr[selected_index]\n",
    "            \n",
    "            fake_hr = Generator.predict(batch_lr)\n",
    "            \n",
    "            real_labels = np.ones(batch_size)\n",
    "            fake_labels = np.zeros(batch_size)\n",
    "            \n",
    "            Discriminator.trainable = True    \n",
    "            d_loss_real = Discriminator.train_on_batch(batch_hr, real_labels)\n",
    "            d_loss_fake = Discriminator.train_on_batch(fake_hr, fake_labels)\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "            \n",
    "            # --------------------\n",
    "            # training Generator\n",
    "            # --------------------\n",
    "            selected_index = np.random.choice(imgs_size,batch_size)\n",
    "            batch_hr, batch_lr = imgs_hr[selected_index], imgs_lr[selected_index]\n",
    "            \n",
    "            _labels_GAN = np.ones(batch_size)\n",
    "            \n",
    "            Discriminator.trainable = False\n",
    "            g_loss = GAN.train_on_batch(batch_lr, [batch_hr, _labels_GAN])\n",
    "            \n",
    "            sum_d_loss += d_loss\n",
    "            sum_GAN_loss += g_loss[0]\n",
    "            \n",
    "            \n",
    "        # print average loss \n",
    "        print(\"discriminator_loss : \", sum_d_loss / float(imgs_size // batch_size))\n",
    "        print(\"GAN_loss : \", sum_GAN_loss / float(imgs_size // batch_size))\n",
    "        \n",
    "        # every show_interval, show(save) img that generator makes.\n",
    "        if e % show_interval == 0:\n",
    "            a = np.random.choice(imgs_size)\n",
    "            hr_test_img, lr_test_img = np.array([imgs_hr[a]]), np.array([imgs_lr[a]])\n",
    "            plot_generated_images('./result', have_trained_epochs+e, Generator, hr_test_img, lr_test_img)\n",
    "        \n",
    "        # every ten epochs, save model.\n",
    "        if e % 100 == 0:\n",
    "            save_model('./Trained_model', Generator, Discriminator)\n",
    "            \n",
    "    return Generator, Discriminator\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baeb5067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------Epoch 1 of 3000---------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [05:58<00:00,  1.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "discriminator_loss :  0.2698075915581285\n",
      "GAN_loss :  0.07819210173562169\n",
      "---------------Epoch 2 of 3000---------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [05:34<00:00,  1.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "discriminator_loss :  0.00045434512632709813\n",
      "GAN_loss :  0.06579980364069343\n",
      "---------------Epoch 3 of 3000---------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [05:33<00:00,  1.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "discriminator_loss :  0.00023418247095833067\n",
      "GAN_loss :  0.05641279364004731\n",
      "---------------Epoch 4 of 3000---------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|███████████████████████████████████████████████████████████████████▏            | 168/200 [04:40<00:53,  1.67s/it]"
     ]
    }
   ],
   "source": [
    "trained_generator, trained_discriminator = train(epochs=3000,\n",
    "                                                 batch_size=4,\n",
    "                                                 show_interval=5)\n",
    "\"\"\"\n",
    ",\n",
    "                                                 give_model=model,\n",
    "                                                 have_trained_epochs=10)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc52066",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
